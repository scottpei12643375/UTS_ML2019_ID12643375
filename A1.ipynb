{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scottpei12643375/UTS_ML2019_ID12643375/blob/master/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaq0nQzmDfj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Review Report on \"FRI01: GREEDY BOOSTING\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjqZ_K_idJ-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Introduction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcHysSyqHEcf",
        "colab_type": "text"
      },
      "source": [
        "This paper describes XGBoost which is an algorithm that dominating applied machine learning and Kaggle competitions for tabular data or structured (Brownlee 2016). The full name of XGBoost is called Extreme Gradient Boosting, there will show more information about what are the goals of project that XGBoost focus on and why it is part of machine learning toolkit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnidiFGTHY-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWLPevcWMeiG",
        "colab_type": "text"
      },
      "source": [
        "The research is about information of XGBoost and how does it achieve features. According to Brownlee (2016), XGBoost is a software library that created by Tianqi Chen. Users can access from interfaces when download and install XGBoost on the machine. Some supported interfaces include C++, JAVA, Python, Command Line Interface (CLI), etc. In this paper, it provides some XGBoost features which are model features, system features and algorithm features. In addition, it also points out two goals or reasons for use XGBoost which are execution speed and model performance. The author provides an algorithm that XGBoost use and it can achieve the two goals that be posted.\n",
        "\n",
        "A theorem has been proved stating that XGBoost always be faster than R, Python Spark and H2O on benchmarked implementations (Pafka 2015). It shows one of the XGBoost goals which is XGBoost Execution Speed. Pafka (2015) performed some tests that compare the performance of XGBoost between with other implementations of gradient boosting and concluded that theorem.\n",
        "\n",
        "Another goal shows XGBoost Model Performance. Brownlee (2016) states that XGBoost dominates tabular datasets on regression and classification predictive modelling questions. It has an evidence show that is first choice from the winner of Kaggle competitive data science platform competition.\n",
        "\n",
        "An algorithm of XGBoost has been proposed, it is using the gradient boosting decision tree algorithm and it has some different names which like multiple additive regression trees, gradient boosting machines, gradient boosting or stochastic gradient boosting (Brownlee 2016). This algorithm is an ensemble technique and it can correct the issues from existing models made when new models are added. New models are added until no further improvements. In addition, this method uses a gradient descent algorithm to decrease the loss when adding new models. \n",
        "\n",
        "At the end of the paper, it shows some resources about information on XGBoost and the links for example codes. Moreover, it has some videos to expand the knowledge of XGBoost and a few summaries for XGBoost algorithm.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr-cCHuE6u-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Innovation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-scBUYyMfDC",
        "colab_type": "text"
      },
      "source": [
        "The creative ideas on XGBoost are features. There are three XGBoost features have been proposed. On model features, it is implementing scikit-learn, R features and regularization. It includes three gradient boosting such as Gradient Boosting, Stochastic Gradient Boosting and Regularized Gradient Boosting. Gradient Boosting algorithm include the learning rate and it can also call gradient boosting machine. Stochastic Gradient Boosting is doing sub-sampling at every split level for row and column. Regularized Gradient Boosting are using L1 and L2 to achieve regularization.\n",
        "\n",
        "According to Brownlee (2016), XGBoost provides a system that use in a computing environment, there are four system features can be used. Firstly, all CPU cores can be used by Parallelization of tree construction during training. Second one is Distributed Computing and it is used by a cluster of machines for training huge models. In addition, one of the features called Out-of-Core Computing and it is focus on large datasets that not suitable for memory. Moreover, last system feature is Cache Optimization. It is suitable on data structures and algorithm to show the hardware functions.\n",
        "\n",
        "Last feature is algorithm features. It can develop calculating time and efficiency of memory resources. There are three key algorithm implementation features such as Block Structure, Continued Training and Sparse Aware. Block Structure supports the parallelization of tree construction, Continued Training can upgrade new data model which was installed and Sparse Aware can process missing data values automatically.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IboRFpC66whN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Technical quality\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5Gvw-nAMfby",
        "colab_type": "text"
      },
      "source": [
        "The author supported their theory using some videos and PowerPoints that list the benefits and feasibility of XGBoost. A Youtube video states that XGBoost is easy to install and highly developed R/Python interface for users. Also, it is efficiency such as it can be run on a cluster and automatic parallel computation on a machine. In addition, XGBoost has a good result for most datasets and it can customize evaluation and objective (NYC Data Science Academy 2015). This video teaches users the basic knowledge of XGBoost such as some basic commands. \n",
        "\n",
        "One of the algorithms for XGBoost is called tree building algorithm (NYC Data Science Academy 2015). It provides two core concepts which are Internal Nodes and Leaves. Internal Nodes describes each internal node split the flow of data points by one of the functions. Leaves means data points reach to a leaf will be assigned a weight and it is the prediction.\n",
        "\n",
        "Another introduction for XGBoost is Parameter. According to NYC Data Science Academy (2015), XGBoost has a rich parameter. It can split three parts which are general parameters, booster parameters and task parameters. General parameters is number of threads and booster parameters are stepsize and regularization. Objective and evaluation metric can be called task parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMWwIjgk6wnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Application and X-factor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8SbdCEPMgED",
        "colab_type": "text"
      },
      "source": [
        "The proposal in the paper promising some methods in XGBoost are useful such as random forests, boosting, neural networks and support vector machines. Especially random forests, it is one of the most accurate algorithms. It has some implementations in Pythn scikit-learn, H2O, R packages, Mahout, Revo ScaleR, Spark MLLib, etc. The interesting process is which tool can deal with 10 million observations in a reasonable period to train a random forest. It can be confirmed by analyse the speed, accuracy and scalability of random forest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yStmbGD6wtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Presentation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkiF9Nw0Mgv4",
        "colab_type": "text"
      },
      "source": [
        "This paper provides a great structure on the goals of XGBoost and how is it work. It can be concluded three points. Firstly, XGBoost is a library for improving fast and high property gradient boosting tree models. Secondly, it can achieve the best performance on some difficult machine learning tasks. Finally, XGBoost can start from Python and R.\n",
        "\n",
        "It could have been more attractive if the author can describe more clearly on some terminologies. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o8bZN5D6wzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Reference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6ABnb84MhZT",
        "colab_type": "text"
      },
      "source": [
        "FRI01:\n",
        "\n",
        "Brownlee, J. 2016, A Gentle Introduction to XGBoost for Applied Machine Learning, viewed 27 August 2019, <https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/>.\n",
        "\n",
        "NYC Data Science Academy 2015, Kaggle Winning Solution Xgboost Algorithm - Learn from Its Author, Tong He, videorecording, Youtube, viewed 28 August 2019, <https://www.youtube.com/watch?v=ufHo8vbk6g4>.\n",
        "\n",
        "Pafka, S. 2015, BENCHMARKING RANDOM FOREST IMPLEMENTATIONS, viewed 27 August 2019, <http://datascience.la/benchmarking-random-forest-implementations/>."
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scottpei12643375/UTS_ML2019_ID12643375/blob/master/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaq0nQzmDfj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Review Report on \"GREEDY BOOSTING\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjqZ_K_idJ-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Introduction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcHysSyqHEcf",
        "colab_type": "text"
      },
      "source": [
        "This paper describes XGBoost which is an algorithm that dominating applied machine learning and Kaggle competitions for tabular data or structured (Brownlee 2016). The full name of XGBoost is called Extreme Gradient Boosting, there will show more information about what are the goals of project that XGBoost focus on and why it is part of machine learning toolkit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnidiFGTHY-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWLPevcWMeiG",
        "colab_type": "text"
      },
      "source": [
        "The research is about information of XGBoost and how does it achieve features. According to Brownlee (2016), XGBoost is a software library that created by Tianqi Chen. Users can access from interfaces when download and install XGBoost on the machine. Some supported interfaces include C++, JAVA, Python, Command Line Interface (CLI), etc. In this paper, it provides some XGBoost features which are model features, system features and algorithm features. In addition, it also points out two goals or reasons for use XGBoost which are execution speed and model performance.\n",
        "\n",
        "An algorithm of XGBoost has been proposed, it is using the gradient boosting decision tree algorithm and it has some different names which like multiple additive regression trees, gradient boosting machines, gradient boosting or stochastic gradient boosting (Brownlee 2016). This algorithm is an ensemble technique and it can correct the issues from existing models made when new models are added. New models are added until no further improvements. In addition, this method uses a gradient descent algorithm to decrease the loss when adding new models. \n",
        "\n",
        "At the end of the paper, it shows some resources about information on XGBoost and the links for example codes. Moreover, it has some videos to expand the knowledge of XGBoost and a few summaries for XGBoost algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr-cCHuE6u-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Innovation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-scBUYyMfDC",
        "colab_type": "text"
      },
      "source": [
        "The creative ideas on XGBoost are features. There are three XGBoost features have been proposed. On model features, it is implementing scikit-learn, R features and regularization. It includes three gradient boosting such as Gradient Boosting, Stochastic Gradient Boosting and Regularized Gradient Boosting. Gradient Boosting algorithm include the learning rate and it can also call gradient boosting machine. Stochastic Gradient Boosting is doing sub-sampling at every split level for row and column. Regularized Gradient Boosting are using L1 and L2 to achieve regularization.\n",
        "\n",
        "According to Brownlee (2016), XGBoost provides a system that use in a computing environment, there are four system features can be used. Firstly, all CPU cores can be used by Parallelization of tree construction during training. Second one is Distributed Computing and it is used by a cluster of machines for training huge models. In addition, one of the features called Out-of-Core Computing and it is focus on large datasets that not suitable for memory. Moreover, last system feature is Cache Optimization. It is suitable on data structures and algorithm to show the hardware functions.\n",
        "\n",
        "Last feature is algorithm features. It can develop calculating time and efficiency of memory resources. There are three key algorithm implementation features such as Block Structure, Continued Training and Sparse Aware. Block Structure supports the parallelization of tree construction, Continued Training can upgrade new data model which was installed and Sparse Aware can process missing data values automatically.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IboRFpC66whN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Technical quality\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5Gvw-nAMfby",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMWwIjgk6wnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Application and X-factor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8SbdCEPMgED",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yStmbGD6wtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Presentation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkiF9Nw0Mgv4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o8bZN5D6wzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Reference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6ABnb84MhZT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}